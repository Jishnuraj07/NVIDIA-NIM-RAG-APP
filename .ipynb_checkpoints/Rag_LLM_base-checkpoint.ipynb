{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905c6786-1884-474f-afc2-f18ff46f0794",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "19dd6c53-ea64-42d7-b526-a084527b2d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "if not os.environ.get(\"NVIDIA_API_KEY\", \"\").startswith(\"nvapi-\"):\n",
    "    nvapi_key = getpass.getpass(\"Enter your NVIDIA API key: \")\n",
    "    assert nvapi_key.startswith(\"nvapi-\"), f\"{nvapi_key[:5]}... is not a valid key\"\n",
    "    os.environ[\"NVIDIA_API_KEY\"] = nvapi_key\n",
    "    os.environ[\"NGC_API_KEY\"] = nvapi_key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f297978e-8694-46e6-a5b4-0bb60d53ebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chains import ConversationalRetrievalChain, LLMChain\n",
    "from langchain.chains.conversational_retrieval.prompts import CONDENSE_QUESTION_PROMPT, QA_PROMPT\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "from pydantic.v1 import BaseModel\n",
    "from langchain_nvidia_ai_endpoints._statics import Model, register_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61e9b299-3005-4e5b-b32f-aab903e60e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab9c760c-58a7-4a2f-b4b1-a7f46e00620d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages/langchain_nvidia_ai_endpoints/_common.py:475: UserWarning: An API key is required for the hosted NIM. This will become an error in the future.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Model(id='deepseek-ai/deepseek-coder-6.7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-deepseek-coder-6_7b-instruct'], supports_tools=False, base_model=None),\n",
       " Model(id='nvidia/neva-22b', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/nvidia/neva-22b', aliases=['ai-neva-22b', 'playground_neva_22b', 'neva_22b'], supports_tools=False, base_model=None),\n",
       " Model(id='google/gemma-2-9b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-2-9b-it'], supports_tools=False, base_model=None),\n",
       " Model(id='snowflake/arctic', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-arctic'], supports_tools=False, base_model=None),\n",
       " Model(id='nvidia/llama3-chatqa-1.5-70b', model_type='qa', client='ChatNVIDIA', endpoint=None, aliases=['ai-chatqa-1.5-70b'], supports_tools=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-small-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-small-128k-instruct'], supports_tools=False, base_model=None),\n",
       " Model(id='adept/fuyu-8b', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/adept/fuyu-8b', aliases=['ai-fuyu-8b', 'playground_fuyu_8b', 'fuyu_8b'], supports_tools=False, base_model=None),\n",
       " Model(id='seallms/seallm-7b-v2.5', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-seallm-7b'], supports_tools=False, base_model=None),\n",
       " Model(id='meta/llama3-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama3-8b'], supports_tools=False, base_model=None),\n",
       " Model(id='mistralai/mistral-large', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-large'], supports_tools=False, base_model=None),\n",
       " Model(id='01-ai/yi-large', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-yi-large'], supports_tools=False, base_model=None),\n",
       " Model(id='writer/palmyra-med-70b-32k', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-palmyra-med-70b-32k'], supports_tools=False, base_model=None),\n",
       " Model(id='writer/palmyra-med-70b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-palmyra-med-70b'], supports_tools=False, base_model=None),\n",
       " Model(id='liuhaotian/llava-v1.6-mistral-7b', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/stg/vlm/community/llava16-mistral-7b', aliases=['ai-llava16-mistral-7b', 'community/llava16-mistral-7b', 'liuhaotian/llava16-mistral-7b'], supports_tools=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-medium-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-medium-128k-instruct'], supports_tools=False, base_model=None),\n",
       " Model(id='mistralai/mixtral-8x7b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mixtral-8x7b-instruct', 'playground_mixtral_8x7b', 'mixtral_8x7b'], supports_tools=False, base_model=None),\n",
       " Model(id='meta/llama2-70b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama2-70b', 'playground_llama2_70b', 'llama2_70b', 'playground_llama2_13b', 'llama2_13b'], supports_tools=False, base_model=None),\n",
       " Model(id='google/codegemma-1.1-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codegemma-1.1-7b'], supports_tools=False, base_model=None),\n",
       " Model(id='google/recurrentgemma-2b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-recurrentgemma-2b'], supports_tools=False, base_model=None),\n",
       " Model(id='nv-mistralai/mistral-nemo-12b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=False, base_model=None),\n",
       " Model(id='meta/llama-3.1-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, base_model=None),\n",
       " Model(id='meta/codellama-70b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codellama-70b', 'playground_llama2_code_70b', 'llama2_code_70b', 'playground_llama2_code_34b', 'llama2_code_34b', 'playground_llama2_code_13b', 'llama2_code_13b'], supports_tools=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-mini-128k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-mini'], supports_tools=False, base_model=None),\n",
       " Model(id='nvidia/nemotron-4-340b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['qa-nemotron-4-340b-instruct'], supports_tools=False, base_model=None),\n",
       " Model(id='databricks/dbrx-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-dbrx-instruct'], supports_tools=False, base_model=None),\n",
       " Model(id='meta/llama3-70b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-llama3-70b'], supports_tools=False, base_model=None),\n",
       " Model(id='mistralai/mistral-7b-instruct-v0.3', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-7b-instruct-v03'], supports_tools=False, base_model=None),\n",
       " Model(id='google/codegemma-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codegemma-7b'], supports_tools=False, base_model=None),\n",
       " Model(id='ibm/granite-8b-code-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-granite-8b-code-instruct'], supports_tools=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-mini-4k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-mini-4k', 'playground_phi2', 'phi2'], supports_tools=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-medium-4k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-medium-4k-instruct'], supports_tools=False, base_model=None),\n",
       " Model(id='mistralai/mistral-7b-instruct-v0.2', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mistral-7b-instruct-v2', 'playground_mistral_7b', 'mistral_7b'], supports_tools=False, base_model=None),\n",
       " Model(id='google/gemma-7b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-7b', 'playground_gemma_7b', 'gemma_7b'], supports_tools=False, base_model=None),\n",
       " Model(id='meta/llama-3.1-405b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, base_model=None),\n",
       " Model(id='microsoft/kosmos-2', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/microsoft/kosmos-2', aliases=['ai-microsoft-kosmos-2', 'playground_kosmos_2', 'kosmos_2'], supports_tools=False, base_model=None),\n",
       " Model(id='google/gemma-2b', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-2b', 'playground_gemma_2b', 'gemma_2b'], supports_tools=False, base_model=None),\n",
       " Model(id='upstage/solar-10.7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-solar-10_7b-instruct'], supports_tools=False, base_model=None),\n",
       " Model(id='mistralai/codestral-22b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-codestral-22b-instruct-v01'], supports_tools=False, base_model=None),\n",
       " Model(id='ibm/granite-34b-code-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-granite-34b-code-instruct'], supports_tools=False, base_model=None),\n",
       " Model(id='meta/llama-3.1-8b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=None, supports_tools=True, base_model=None),\n",
       " Model(id='mistralai/mixtral-8x22b-instruct-v0.1', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-mixtral-8x22b-instruct'], supports_tools=False, base_model=None),\n",
       " Model(id='liuhaotian/llava-v1.6-34b', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/stg/vlm/community/llava16-34b', aliases=['ai-llava16-34b', 'community/llava16-34b', 'liuhaotian/llava16-34b'], supports_tools=False, base_model=None),\n",
       " Model(id='google/gemma-2-27b-it', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-gemma-2-27b-it'], supports_tools=False, base_model=None),\n",
       " Model(id='mediatek/breeze-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-breeze-7b-instruct'], supports_tools=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-vision-128k-instruct', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/microsoft/phi-3-vision-128k-instruct', aliases=['ai-phi-3-vision-128k-instruct'], supports_tools=False, base_model=None),\n",
       " Model(id='nvidia/llama3-chatqa-1.5-8b', model_type='qa', client='ChatNVIDIA', endpoint=None, aliases=['ai-chatqa-1.5-8b'], supports_tools=False, base_model=None),\n",
       " Model(id='microsoft/phi-3-small-8k-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-phi-3-small-8k-instruct'], supports_tools=False, base_model=None),\n",
       " Model(id='aisingapore/sea-lion-7b-instruct', model_type='chat', client='ChatNVIDIA', endpoint=None, aliases=['ai-sea-lion-7b-instruct'], supports_tools=False, base_model=None),\n",
       " Model(id='google/deplot', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/google/deplot', aliases=['ai-google-deplot', 'playground_deplot', 'deplot'], supports_tools=False, base_model=None),\n",
       " Model(id='google/paligemma', model_type='vlm', client='ChatNVIDIA', endpoint='https://ai.api.nvidia.com/v1/vlm/google/paligemma', aliases=['ai-google-paligemma'], supports_tools=False, base_model=None)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ChatNVIDIA.get_available_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86200f9e-9a86-464d-9219-779d98c6a3b5",
   "metadata": {},
   "source": [
    "<h3>\n",
    " Hands-on section    \n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c28f0801-23ac-497b-af06-9f3a275692ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import NVIDIAEmbeddings\n",
    "\n",
    "embeddingmodels = NVIDIAEmbeddings(model=\"NV-Embed-QA\", truncate=\"END\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "25270a1f-3703-48fb-b485-1425c15fe7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nvidia was founded on April 5, 1993, by Jensen Huang, Chris Malachowsky, Curtis Priem, and Ray Bingham. As of 2022, the CEO is Jensen Huang, who has been leading the company since co-founding it.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"https://integrate.api.nvidia.com/v1/chat/completions\"\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"Authorization\": \"Bearer {}\".format(os.environ[\"NVIDIA_API_KEY\"])\n",
    "}\n",
    "data = {\n",
    "    \"model\": \"mistralai/mistral-7b-instruct-v0.3\",\n",
    "    \"messages\": [{\"role\": \"user\", \"content\": \"When was Nvidia founded and who is the CEO?\"}],\n",
    "    \"temperature\": 0.5,\n",
    "    \"top_p\": 0.7,\n",
    "    \"max_tokens\": 1024,\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = requests.post(url, headers=headers, data=json.dumps(data))\n",
    "\n",
    "print(response.json()['choices'][0]['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1e6b50a5-739b-4a2d-9084-94d7f0ccfe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "model = \"mistralai/mistral-7b-instruct-v0.3\"\n",
    "llm = ChatNVIDIA(model=model, max_tokens=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0f7b1b1b-de20-448c-864c-4d80bf2ae818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"In the context of project management, a RAG rating is a quick and simple way to communicate a project's current status using three categories: Red, Amber, Green.\\n\\n1. Red: This indicates a problem or risk that needs immediate attention. The project is not on target, and corrective action is needed.\\n\\n2. Amber: This signifies that there are issues, but they are not yet critical. The project is facing challenges, but there are opportunities to\", response_metadata={'role': 'assistant', 'content': \"In the context of project management, a RAG rating is a quick and simple way to communicate a project's current status using three categories: Red, Amber, Green.\\n\\n1. Red: This indicates a problem or risk that needs immediate attention. The project is not on target, and corrective action is needed.\\n\\n2. Amber: This signifies that there are issues, but they are not yet critical. The project is facing challenges, but there are opportunities to\", 'token_usage': {'prompt_tokens': 9, 'total_tokens': 109, 'completion_tokens': 100}, 'finish_reason': 'length', 'model_name': 'mistralai/mistral-7b-instruct-v0.3'}, id='run-d3694d8e-8b01-4c89-bd62-d856dfc5c3d5-0', role='assistant')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = llm.invoke(\"What is a RAG?\")\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "ce2ac9b4-dab5-41ac-b6ae-83c18b553f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\"https://www.nvidia.com/en-in/glossary/retrieval-augmented-generation/\",\n",
    "       \"https://en.wikipedia.org/wiki/Retrieval-augmented_generation\",\n",
    "        \"https://docs.nvidia.com/cuda/\",\n",
    "        \"https://github.com/NVIDIA/cuda-samples\"\n",
    "        \"https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html\"\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4f4d12-40b5-4e13-8bcd-64b80a79ae74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List, Union\n",
    "\n",
    "def html_document_loader(url: Union[str, bytes]) -> str:\n",
    "    \"\"\"\n",
    "    Loads the HTML content of a document from a given URL and return it's content.\n",
    "\n",
    "    Args:\n",
    "        url: The URL of the document.\n",
    "\n",
    "    Returns:\n",
    "        The content of the document.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If there is an error while making the HTTP request.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        html_content = response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {url} due to exception {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        # Create a Beautiful Soup object to parse html\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Remove script and style tags\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "\n",
    "        # Get the plain text from the HTML document\n",
    "        text = soup.get_text()\n",
    "\n",
    "        # Remove excess whitespace and newlines\n",
    "        text = re.sub(\"\\s+\", \" \", text).strip()\n",
    "\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Exception {e} while loading document\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ee65fd-acd1-4607-b5bf-3c7c15f5a2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(embeddings_model,embedding_path: str = \"./embed\"):\n",
    "\n",
    "    embedding_path = \"./embed\"\n",
    "    print(f\"Storing embeddings to {embedding_path}\")\n",
    "\n",
    "    documents = []\n",
    "    for url in urls:\n",
    "        document = html_document_loader(url)\n",
    "        documents.append(document)\n",
    "\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=0,\n",
    "        length_function=len,\n",
    "    )\n",
    "    print(\"Total documents:\",len(documents))\n",
    "    texts = text_splitter.create_documents(documents)\n",
    "    print(\"Total texts:\",len(texts))\n",
    "    index_docs(embeddings_model,url, text_splitter, texts, embedding_path,)\n",
    "    print(\"Generated embedding successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc43d37a-36e7-4e16-b1a3-f12eef075d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "import os\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "def index_docs(embeddings_model, url: Union[str, bytes], splitter, documents: List[str], dest_embed_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Split the documents into chunks and create embeddings for them.\n",
    "    \n",
    "    Args:\n",
    "        embeddings_model: Model used for creating embeddings.\n",
    "        url: Source url for the documents.\n",
    "        splitter: Splitter used to split the documents.\n",
    "        documents: List of documents whose embeddings need to be created.\n",
    "        dest_embed_dir: Destination directory for embeddings.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    metadatas = []\n",
    "\n",
    "    for document in documents:\n",
    "        chunk_texts = splitter.split_text(document.page_content)\n",
    "        texts.extend(chunk_texts)\n",
    "        metadatas.extend([document.metadata] * len(chunk_texts))\n",
    "\n",
    "    if os.path.exists(dest_embed_dir):\n",
    "        docsearch = FAISS.load_local(\n",
    "            folder_path=dest_embed_dir, \n",
    "            embeddings=embeddings_model, \n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        docsearch.add_texts(texts, metadatas=metadatas)\n",
    "    else:\n",
    "        docsearch = FAISS.from_texts(texts, embedding=embeddings_model, metadatas=metadatas)\n",
    "\n",
    "    docsearch.save_local(folder_path=dest_embed_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415939a6-7c62-40dd-8cd5-e6f44c1aff81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "create_embeddings(embeddings_model=embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b03145-0517-4aba-816d-4be1e66d4880",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_path = \"./embed/\"\n",
    "docsearch = FAISS.load_local(folder_path=embedding_path, embeddings=embeddings_model, allow_dangerous_deserialization=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f987ffd0-883b-46e6-9343-5340b5fe77ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_llm = ChatNVIDIA(model=\"meta/llama-3.1-70b-instruct\")\n",
    "chat_llm = ChatNVIDIA(model=\"meta/llama3-8b-instruct\",\n",
    "                      temperature=0.1,\n",
    "                      max_tokens=1000,\n",
    "                      top_p=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01041cbd-d99e-426a-b4ab-c68bb6c8d63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import (\n",
    "create_history_aware_retriever,\n",
    "create_retrieval_chain)\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.chat_history import BaseChatMessageHistory\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d5f72b-f1de-45af-a37a-d634bda0efdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setup Message History format and retriever\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "#Setup Agent Behaviour\n",
    "qa_system_prompt = \"\"\"You are an assistant for question-answering tasks. \\\n",
    "Try to use the following pieces of retrieved context to answer the question. \\\n",
    "Use three sentences maximum and keep the answer concise.\\\n",
    "\n",
    "{context}\"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26561dd3-c47d-4858-b284-a1a0acd06bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    summary_llm, \n",
    "    docsearch.as_retriever(), # the vectorstore serves as the junction to retrieve documents with highest similarity to the query.\n",
    "    contextualize_q_prompt\n",
    ")\n",
    "question_answer_chain = create_stuff_documents_chain(chat_llm, qa_prompt)\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47974e97-c799-4395-ab38-58be6f9295c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample session creation\n",
    "store = {}\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57bcf6e-8984-47ac-bbf5-5bad0a86f6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What is RAG?\"},\n",
    "     config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    }\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5cfab2-77d9-4fd8-a44f-6e2e667de88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"How is it helpful?\"},\n",
    "     config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    }\n",
    ")[\"answer\"]\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42dd334-7cfc-4629-8072-f79063aea138",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What is the meaning of retrieval?\"},\n",
    "     config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    }\n",
    ")[\"answer\"]\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7f53fd-49e1-4d75-95be-c9dc2f652661",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = conversational_rag_chain.invoke(\n",
    "    {\"input\": \"Can you write a kernel to add the elements of two arrays and store the output in a third array?\"},\n",
    "     config={\n",
    "        \"configurable\": {\"session_id\": \"xyz456\"}\n",
    "    }\n",
    ")[\"answer\"]\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32723aa7-9b68-4cfb-997f-430de0268cdc",
   "metadata": {},
   "source": [
    "<h3>Initialising \"meta / llama-3.1-8b-instruct\" LLM model to the cluster</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9b4043c-0e60-4753-9ec5-c31fce3da00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID                                                       IMAGE                                                COMMAND                                                       CREATED        STATUS                  PORTS                                         NAMES\n",
      "4ad7b8ee2bb5f14cde2e3e60e4eafad6e0b4b61a78670deb441a0755c90392df   nvcr.io/nim/nvidia/nv-embedqa-e5-v5:1.0.1            \"/opt/nvidia/nvidia_entrypoint.sh /opt/nim/start-server.sh\"   46 hours ago   Up 46 hours             0.0.0.0:11022->8000/tcp, :::11022->8000/tcp   embed_nim\n",
      "6ef6ad73da5540b0dd28c15da4bf2ab25fc02cc7d2432b495bfdb0295bcfb7f8   nvcr.io/nim/nvidia/nv-rerankqa-mistral-4b-v3:1.0.2   \"/opt/nvidia/nvidia_entrypoint.sh /opt/nim/start-server.sh\"   46 hours ago   Up 46 hours             0.0.0.0:11737->8000/tcp, :::11737->8000/tcp   rerank_nim\n",
      "a5732a8ccde9ca1fbf867980c23f1dd95424b2072b1f6f77c6f408aa39f48ad5   nvcr.io/nim/meta/llama-3.1-8b-instruct:1.1.2         \"/opt/nvidia/nvidia_entrypoint.sh /opt/nim/start-server.sh\"   47 hours ago   Up 47 hours             0.0.0.0:8000->8000/tcp, :::8000->8000/tcp     LLM_nim\n",
      "40905e6dfcd07b5c165984073903a45c873acf5b7e1d2516d464a0b1e078f481   nvcr.io/nim/meta/llama-3.1-8b-instruct:1.1.2         \"/opt/nvidia/nvidia_entrypoint.sh list-model-profiles\"        2 days ago     Exited (0) 2 days ago                                                 nostalgic_dijkstra\n",
      "c58e6d81a4a37eb99549e91398498b336ba905342b6eb70834a0d93b53a16998   nvcr.io/nim/meta/llama-3.1-8b-instruct:1.1.2         \"/opt/nvidia/nvidia_entrypoint.sh list-model-profiles\"        2 days ago     Exited (0) 2 days ago                                                 pedantic_bassi\n",
      "0ddcf40ea269d02fdd22cc54d45c7d874a812a4330cec36a5e6f4e74fc85dbad   nvcr.io/nim/meta/llama-3.1-8b-instruct:1.1.2         \"/opt/nvidia/nvidia_entrypoint.sh list-model-profiles\"        2 days ago     Exited (0) 2 days ago                                                 beautiful_bassi\n"
     ]
    }
   ],
   "source": [
    "! docker ps -a --no-trunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "353ce554-7b42-4b26-9c38-3feccac036b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2004hI have no name!@a5732a8ccde9:/$ ^C\u001b[?2004l\n",
      "\u001b[?2004l\n",
      "\u001b[?2004hI have no name!@a5732a8ccde9:/$ "
     ]
    }
   ],
   "source": [
    " !docker exec -it a5732a8ccde9 bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e1815bcb-9f15-49fb-bef3-3fbdb5483176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING! Your password will be stored unencrypted in /home/gsh-3atzc7/.docker/config.json.\n",
      "Configure a credential helper to remove this warning. See\n",
      "https://docs.docker.com/engine/reference/commandline/login/#credentials-store\n",
      "\n",
      "Login Succeeded\n"
     ]
    }
   ],
   "source": [
    "! echo -e \"$NGC_API_KEY\" | docker login nvcr.io --username '$oauthtoken' --password-stdin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9293a4b9-1fc7-463e-8d91-be72f784caec",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker pull nvcr.io/nim/meta/llama-3.1-8b-instruct:1.1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75ab5731-7bc0-4856-a830-6bc52f7042fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPOSITORY                                     TAG       IMAGE ID                                                                  CREATED        SIZE\n",
      "nvcr.io/nim/meta/llama-3.1-8b-instruct         1.1.2     sha256:c9102c3e95f3acae5b41fdafd7aefd3389bdaf33cf0b3458041e45046bf6bb1d   7 weeks ago    12.9GB\n",
      "nvcr.io/nim/nvidia/nv-rerankqa-mistral-4b-v3   1.0.2     sha256:47df3d3998eb2f8fcf3da6c8aa55297b34c4789234ea0f2cbf54b4bcd411b37d   2 months ago   15.5GB\n",
      "nvcr.io/nim/nvidia/nv-embedqa-e5-v5            1.0.1     sha256:fa5c1fc5ccb39cd6d9910ac04fb8bda7ea46e6f82de8d05c2972d1071d5a62c7   2 months ago   15.7GB\n"
     ]
    }
   ],
   "source": [
    "! docker image ls -a --no-trunc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85e28fd-7145-4c9d-bffe-906802d8f744",
   "metadata": {},
   "outputs": [],
   "source": [
    "!export NGC_API_KEY=\"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "130e6259-70d2-4d82-8f96-e9d7fce5f562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/gsh-3atzc7/.cache/nim\n"
     ]
    }
   ],
   "source": [
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "os.environ['LOCAL_NIM_CACHE']=f\"{home}/.cache/nim\"\n",
    "!echo $LOCAL_NIM_CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86505154-8d5d-426b-9f80-e0ef09920446",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p \"$LOCAL_NIM_CACHE\"\n",
    "!chmod 777 \"$LOCAL_NIM_CACHE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad59c68-c289-4a98-aaf0-4783fd46dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import socket\n",
    "\n",
    "def find_available_port(start=11000, end=11999):\n",
    "    while True:\n",
    "        # Randomly select a port between start and end range\n",
    "        port = random.randint(start, end)\n",
    "        \n",
    "        # Try to create a socket and bind to the port\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "            try:\n",
    "                sock.bind((\"localhost\", port))\n",
    "                # If binding is successful, the port is free\n",
    "                return port\n",
    "            except OSError:\n",
    "                # If binding fails, the port is in use, continue to the next iteration\n",
    "                continue\n",
    "\n",
    "# Find and print an available port\n",
    "os.environ['CONTAINER_PORT'] = str(find_available_port())\n",
    "print(f\"Your have been alloted the available port: {os.environ['CONTAINER_PORT']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547e681b-5964-4a90-9287-12b86c951f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker run -it -d --rm \\\n",
    "--gpus 1 \\\n",
    "--name=LLM_nim \\\n",
    "--shm-size=16GB \\\n",
    "-e NGC_API_KEY \\\n",
    "-v $LOCAL_NIM_CACHE:/opt/nim/.cache \\\n",
    "-u $(id -u) \\\n",
    "-p 8000:8000 \\\n",
    "nvcr.io/nim/meta/llama-3.1-8b-instruct:1.1.2\n",
    "# In order to ensure, the local NIM container is completely loaded and doesn't remain in pending stage, we instantiate a wait interval\n",
    "! sleep 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6428ce63-6637-4e08-8747-4222797b8db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker logs --details LLM_nim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c747b6a9-1605-4511-87a5-2202aa55a7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
    "\n",
    "llm = ChatNVIDIA(base_url= \"http://0.0.0.0:8000/v1\", model=\"meta/llama-3.1-8b-instruct\", temperature=0.1, max_tokens=1000, top_p=1.0)\n",
    "\n",
    "result = llm.invoke(\"What is the capital of France?\")\n",
    "print(result.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "64430479-7f01-4671-b029-30ea265d9d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\":\"cmpl-841c3b6fea2d4ec8a3db6ae681e1c24a\",\"object\":\"text_completion\",\"created\":1728051640,\"model\":\"meta/llama-3.1-8b-instruct\",\"choices\":[{\"index\":0,\"text\":\" [Answer]\\nThe capital of France is Paris.\\nLooking for somewhere to eat in Paris?\\nWhy the Eiffel Tower is a must visit.\\nPractical information about visiting France. [Answer]\\nLanguage : French\\nCurrency: Euro\\nVisa requirements: Check with your home country's government on the current visa requirements\\n\",\"logprobs\":null,\"finish_reason\":\"length\",\"stop_reason\":null}],\"usage\":{\"prompt_tokens\":8,\"total_tokens\":72,\"completion_tokens\":64}}"
     ]
    }
   ],
   "source": [
    "!curl -X 'POST' \\\n",
    "    \"http://0.0.0.0:8000/v1/completions\" \\\n",
    "    -H \"accept: application/json\" \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\"model\": \"meta/llama-3.1-8b-instruct\", \"prompt\": \"What is the capital of France?\", \"max_tokens\": 64}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ecb50b-7608-4a78-8fd9-d759201f97f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker ps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94905de9-aedc-47a9-a3ec-8044f9a8889a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker container stop rerank_nim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffef4fa5-4b81-4bb5-a516-c2c2b9b53820",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker container stop embed_nim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2c9b6c-cad3-42e4-9ba5-93a9bc9bba8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run nvcr.io/nim/meta/llama-3.1-8b-instruct:1.1.2 list-model-profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17cd868-7784-478e-a429-fa9dc63f6882",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker ps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba6f5f5-c4e7-4549-9c74-58ff7d7edd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1622810f-e7bc-4c9b-9430-7a8737c15f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "os.environ['LOCAL_NIM_CACHE']=f\"{home}/.cache/nim\"\n",
    "!echo $LOCAL_NIM_CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af7a9314-f2be-4346-bbd5-8a326e9bae56",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p \"$LOCAL_NIM_CACHE\"\n",
    "# !chmod 777 \"$LOCAL_NIM_CACHE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abeeaf7-00a5-40e7-876d-e95ba01d6f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import socket\n",
    "\n",
    "def find_available_port(start=11000, end=11999):\n",
    "    while True:\n",
    "        # Randomly select a port between start and end range\n",
    "        port = random.randint(start, end)\n",
    "        \n",
    "        # Try to create a socket and bind to the port\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "            try:\n",
    "                sock.bind((\"localhost\", port))\n",
    "                # If binding is successful, the port is free\n",
    "                return port\n",
    "            except OSError:\n",
    "                # If binding fails, the port is in use, continue to the next iteration\n",
    "                continue\n",
    "\n",
    "# Find and print an available port\n",
    "os.environ['CONTAINER_PORT'] = str(find_available_port())\n",
    "print(f\"Your have been alloted the available port: {os.environ['CONTAINER_PORT']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e413e25b-a989-49e4-8b5f-331ed29db701",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker run -it -d --rm \\\n",
    "--gpus device=1 \\\n",
    "--name=embed_nim \\\n",
    "--shm-size=16GB  \\\n",
    "-e NGC_API_KEY \\\n",
    "-v $LOCAL_NIM_CACHE:/opt/nim/.cache \\\n",
    "-u $(id -u) \\\n",
    "-p $CONTAINER_PORT:8000 \\\n",
    "nvcr.io/nim/nvidia/nv-embedqa-e5-v5:1.0.1\n",
    "\n",
    "# In order to ensure, the local NIM container is completely loaded and doesn't remain in pending stage, we instantiate a wait interval\n",
    "! sleep 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b17f422-2e52-468d-8bcb-79849f015188",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker ps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251f350a-3834-4960-bc47-c63f7d1fada6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker container stop embed_nim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747c9c85-a684-48e2-a74a-e9d861172130",
   "metadata": {},
   "source": [
    "<h3>Deploying embedding and retriever models locally</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2761dcac-fdea-4475-98f9-623c494ed308",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker pull nvcr.io/nim/nvidia/nv-rerankqa-mistral-4b-v3:1.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0681212-435d-432b-81bd-ffc0db7f0978",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import expanduser\n",
    "home = expanduser(\"~\")\n",
    "os.environ['LOCAL_NIM_CACHE']=f\"{home}/.cache/nim\"\n",
    "!echo $LOCAL_NIM_CACHE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdc598e-a070-47cb-b03d-9c5f63705779",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p \"$LOCAL_NIM_CACHE\"\n",
    "# !chmod 777 \"$LOCAL_NIM_CACHE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6576e7a2-c496-44ed-b085-e1fbd3f6ea00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import socket\n",
    "\n",
    "def find_available_port(start=11000, end=11999):\n",
    "    while True:\n",
    "        # Randomly select a port between start and end range\n",
    "        port = random.randint(start, end)\n",
    "        \n",
    "        # Try to create a socket and bind to the port\n",
    "        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n",
    "            try:\n",
    "                sock.bind((\"localhost\", port))\n",
    "                # If binding is successful, the port is free\n",
    "                return port\n",
    "            except OSError:\n",
    "                # If binding fails, the port is in use, continue to the next iteration\n",
    "                continue\n",
    "\n",
    "# Find and print an available port\n",
    "os.environ['CONTAINER_PORT'] = str(find_available_port())\n",
    "print(f\"Your have been alloted the available port: {os.environ['CONTAINER_PORT']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ad5936-904e-49f3-b7f6-ff2a61d7152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "! docker run -it -d --rm \\\n",
    "--gpus device=2  \\\n",
    "--name=rerank_nim \\\n",
    "--shm-size=16GB  \\\n",
    "-e NGC_API_KEY \\\n",
    "-v $LOCAL_NIM_CACHE:/opt/nim/.cache \\\n",
    "-u $(id -u) \\\n",
    "-p $CONTAINER_PORT:8000 \\\n",
    "nvcr.io/nim/nvidia/nv-rerankqa-mistral-4b-v3:1.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "544a8cb7-8404-46a5-80e8-d5e381dc1962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE                                                COMMAND                  CREATED       STATUS       PORTS                                         NAMES\n",
      "4ad7b8ee2bb5   nvcr.io/nim/nvidia/nv-embedqa-e5-v5:1.0.1            \"/opt/nvidia/nvidia_…\"   7 hours ago   Up 7 hours   0.0.0.0:11022->8000/tcp, :::11022->8000/tcp   embed_nim\n",
      "6ef6ad73da55   nvcr.io/nim/nvidia/nv-rerankqa-mistral-4b-v3:1.0.2   \"/opt/nvidia/nvidia_…\"   7 hours ago   Up 7 hours   0.0.0.0:11737->8000/tcp, :::11737->8000/tcp   rerank_nim\n",
      "a5732a8ccde9   nvcr.io/nim/meta/llama-3.1-8b-instruct:1.1.2         \"/opt/nvidia/nvidia_…\"   8 hours ago   Up 8 hours   0.0.0.0:8000->8000/tcp, :::8000->8000/tcp     LLM_nim\n"
     ]
    }
   ],
   "source": [
    "!docker ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e699f6-8510-4096-8e2d-156e3e36e0a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "03bd7195-f2c5-45e1-8913-deb3d0292a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = [\"https://www.nvidia.com/en-in/glossary/retrieval-augmented-generation/\",\n",
    "       \"https://en.wikipedia.org/wiki/Retrieval-augmented_generation\",\n",
    "        \"https://docs.nvidia.com/cuda/\",\n",
    "        \"https://github.com/NVIDIA/cuda-samples\"\n",
    "        \"https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html\"\n",
    "       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "111d5f1e-c91e-4b27-98f9-2965b26e84d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List, Union\n",
    "\n",
    "def html_document_loader(url: Union[str, bytes]) -> str:\n",
    "    \"\"\"\n",
    "    Loads the HTML content of a document from a given URL and return it's content.\n",
    "\n",
    "    Args:\n",
    "        url: The URL of the document.\n",
    "\n",
    "    Returns:\n",
    "        The content of the document.\n",
    "\n",
    "    Raises:\n",
    "        Exception: If there is an error while making the HTTP request.\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        html_content = response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {url} due to exception {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        # Create a Beautiful Soup object to parse html\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "        # Remove script and style tags\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "\n",
    "        # Get the plain text from the HTML document\n",
    "        text = soup.get_text()\n",
    "\n",
    "        # Remove excess whitespace and newlines\n",
    "        text = re.sub(\"\\s+\", \" \", text).strip()\n",
    "\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Exception {e} while loading document\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2be7e048-fa6e-4b95-97dc-a745c53732e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(embeddings_model,embedding_path: str = \"./embed\"):\n",
    "\n",
    "    embedding_path = \"./embed\"\n",
    "    print(f\"Storing embeddings to {embedding_path}\")\n",
    "\n",
    "    documents = []\n",
    "    for url in urls:\n",
    "        document = html_document_loader(url)\n",
    "        documents.append(document)\n",
    "\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=0,\n",
    "        length_function=len,\n",
    "    )\n",
    "    print(\"Total documents:\",len(documents))\n",
    "    texts = text_splitter.create_documents(documents)\n",
    "    print(\"Total texts:\",len(texts))\n",
    "    index_docs(embeddings_model,url, text_splitter, texts, embedding_path,)\n",
    "    print(\"Generated embedding successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bed5925f-718d-481c-9374-749d3b4dcfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = NVIDIAEmbeddings(base_url=\"http://0.0.0.0:11022/v1\", model='nvidia/nv-embedqa-e5-v5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7ecfac-ff25-4758-8e97-222330d2bb97",
   "metadata": {},
   "source": [
    "<h3>Faiss vector store through langchain</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7924a57b-b174-4855-81ec-b6482e242788",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "import os\n",
    "from langchain.vectorstores import FAISS\n",
    "# model_kwargs = {\"device\": \"cuda\"}\n",
    "\n",
    "def index_docs(embeddings_model, url: Union[str, bytes], splitter, documents: List[str], dest_embed_dir: str) -> None:\n",
    "    \"\"\"\n",
    "    Split the documents into chunks and create embeddings for them.\n",
    "    \n",
    "    Args:\n",
    "        embeddings_model: Model used for creating embeddings.\n",
    "        url: Source url for the documents.\n",
    "        splitter: Splitter used to split the documents.\n",
    "        documents: List of documents whose embeddings need to be created.\n",
    "        dest_embed_dir: Destination directory for embeddings.\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    metadatas = []\n",
    "\n",
    "    for document in documents:\n",
    "        chunk_texts = splitter.split_text(document.page_content)\n",
    "        texts.extend(chunk_texts)\n",
    "        metadatas.extend([document.metadata] * len(chunk_texts))\n",
    "\n",
    "    if os.path.exists(dest_embed_dir):\n",
    "        docsearch = FAISS.load_local(\n",
    "            folder_path=dest_embed_dir, \n",
    "            embeddings=embeddings_model, \n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        docsearch.add_texts(texts, metadatas=metadatas)\n",
    "    else:\n",
    "        docsearch = FAISS.from_texts(texts, embedding=embeddings_model, metadatas=metadatas,)\n",
    "\n",
    "    docsearch.save_local(folder_path=dest_embed_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6104b9eb-9f0c-4c69-83e2-acdaac4eb3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing embeddings to ./embed\n",
      "Processing PDF: CUDA_C_Programming_Guide.pdf\n",
      "Total documents: 6\n",
      "Total texts: 3026\n",
      "Generated embedding successfully\n",
      "CPU times: user 2.67 s, sys: 181 ms, total: 2.85 s\n",
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "create_embeddings(embeddings_model=embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cdc771-0683-471b-9c9a-903516a273a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "44f15b99-8603-4a79-b0be-2a61cff972d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load Embed documents\n",
    "embedding_path = \"./embed/\"\n",
    "docsearch = FAISS.load_local(folder_path=embedding_path, embeddings=embeddings_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d12210-4d9c-4c10-8316-a2fe51b57c9c",
   "metadata": {},
   "source": [
    "<h3>HTML and pdf embedding </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b25004cc-d029-44a8-a06c-40313a62ee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import requests\n",
    "import pymupdf  # Explicitly using pymupdf instead of fitz\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import List, Union\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import UnstructuredFileLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# List of URLs\n",
    "urls = [\n",
    "    \"https://www.nvidia.com/en-in/glossary/retrieval-augmented-generation/\",\n",
    "    \"https://en.wikipedia.org/wiki/Retrieval-augmented_generation\",\n",
    "    \"https://docs.nvidia.com/cuda/\",\n",
    "    \"https://github.com/NVIDIA/cuda-samples\",\n",
    "    \"https://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html\",\n",
    "    \"https://developer.nvidia.com/blog/efficient-cuda-debugging-using-compute-sanitizer-with-nvtx-and-creating-custom-tools/\",\n",
    "    \"https://developer.nvidia.com/blog/debugging-cuda-more-efficiently-with-nvidia-compute-sanitizer/\",\n",
    "    \"https://developer.nvidia.com/blog/multi-gpu-programming-with-standard-parallel-c-part-1/\",\n",
    "    \"https://developer.nvidia.com/blog/multi-gpu-programming-with-standard-parallel-c-part-2\",\n",
    "    \"https://developer.nvidia.com/blog/efficient-cuda-debugging-memory-initialization-and-thread-synchronization-with-nvidia-compute-sanitizer/\",\n",
    "    \"https://cuda-tutorial.readthedocs.io/en/latest/tutorials/tutorial02/#introduction\",\n",
    "    \"https://developer.nvidia.com/blog/even-easier-introduction-cuda/\",\n",
    "    \"https://github.com/jkonvicka/Nvidia-CUDA-course/blob/main/AC_CUDA_C.md\",\n",
    "    \"https://github.com/jkonvicka/Nvidia-CUDA-course/blob/main/cuda_cheatsheet.md\",\n",
    "    \"https://github.com/jkonvicka/Nvidia-CUDA-course/blob/main/Unified%20Memory.md\",\n",
    "    \"https://github.com/jkonvicka/Nvidia-CUDA-course/blob/main/Streaming%20and%20Visual%20Profiling.md\",\n",
    "    \"https://github.com/jkonvicka/Nvidia-CUDA-course/blob/main/AC_CUDA_C.md\",\n",
    "    \"https://tbetcke.github.io/hpc_lecture_notes/intro.html\",\n",
    "    \n",
    "]\n",
    "\n",
    "# Directory containing PDF files\n",
    "pdf_dir = '/mnt/lustre/hackathons/hack_teams/hack_team_16/workspace/Jishnu/Pdf'\n",
    "\n",
    "# Directory containing other file types (e.g., .txt, .docx)\n",
    "other_files_dir = '/mnt/lustre/hackathons/hack_teams/hack_team_16/workspace/Jishnu/text'\n",
    "\n",
    "# Function to load HTML content from a URL\n",
    "def html_document_loader(url: Union[str, bytes]) -> str:\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        html_content = response.text\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {url} due to exception {e}\")\n",
    "        return \"\"\n",
    "\n",
    "    try:\n",
    "        soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "        # Remove script and style tags\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "        text = soup.get_text()\n",
    "        text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "        return text if isinstance(text, str) else \"\"\n",
    "    except Exception as e:\n",
    "        print(f\"Exception {e} while loading document\")\n",
    "        return \"\"\n",
    "\n",
    "# Function to extract text from PDFs using PyMuPDF\n",
    "def extract_text_from_pdfs(pdf_directory: str) -> List[dict]:\n",
    "    pdf_texts = []\n",
    "    \n",
    "    for pdf_file in os.listdir(pdf_directory):\n",
    "        if pdf_file.lower().endswith('.pdf'):\n",
    "            pdf_path = os.path.join(pdf_directory, pdf_file)\n",
    "            print(f\"Processing PDF: {pdf_file}\")\n",
    "\n",
    "            # Open the PDF file\n",
    "            with pymupdf.open(pdf_path) as doc:\n",
    "                extracted_text = \"\"\n",
    "                for page_num in range(len(doc)):\n",
    "                    page = doc.load_page(page_num)\n",
    "                    extracted_text += page.get_text()\n",
    "            \n",
    "            # Ensure extracted text is a string\n",
    "            extracted_text = extracted_text if isinstance(extracted_text, str) else \"\"\n",
    "            \n",
    "            # Append extracted text and metadata\n",
    "            pdf_texts.append({\n",
    "                'page_content': extracted_text,\n",
    "                'metadata': {'type': 'pdf', 'source': pdf_file}\n",
    "            })\n",
    "\n",
    "    return pdf_texts\n",
    "\n",
    "# Function to handle other file types using UnstructuredFileLoader\n",
    "def extract_text_from_other_files(file_directory: str) -> List[dict]:\n",
    "    other_texts = []\n",
    "    \n",
    "    for file_name in os.listdir(file_directory):\n",
    "        file_path = os.path.join(file_directory, file_name)\n",
    "        # Use UnstructuredFileLoader for other file types\n",
    "        loader = UnstructuredFileLoader(file_path)\n",
    "        try:\n",
    "            loaded_documents = loader.load()\n",
    "            for doc in loaded_documents:\n",
    "                other_texts.append({\n",
    "                    'page_content': doc.page_content,\n",
    "                    'metadata': {'type': 'file', 'source': file_name}\n",
    "                })\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load file {file_name} due to exception: {e}\")\n",
    "    \n",
    "    return other_texts\n",
    "\n",
    "# Function to index documents using embeddings\n",
    "def index_docs(embeddings_model, url: Union[str, bytes], splitter, documents: List[object], dest_embed_dir: str) -> None:\n",
    "    texts = []\n",
    "    metadatas = []\n",
    "\n",
    "    for document in documents:\n",
    "        # Access the content and metadata of each Document object\n",
    "        page_content = document.page_content\n",
    "        metadata = document.metadata\n",
    "        \n",
    "        # Ensure document content is a string\n",
    "        if not isinstance(page_content, str):\n",
    "            print(f\"Skipping document due to invalid content: {document}\")\n",
    "            continue\n",
    "        \n",
    "        chunk_texts = splitter.split_text(page_content)\n",
    "        texts.extend(chunk_texts)\n",
    "        metadatas.extend([metadata] * len(chunk_texts))\n",
    "\n",
    "    # Load existing FAISS index or create a new one if it doesn't exist\n",
    "    if os.path.exists(dest_embed_dir):\n",
    "        docsearch = FAISS.load_local(\n",
    "            folder_path=dest_embed_dir, \n",
    "            embeddings=embeddings_model, \n",
    "            allow_dangerous_deserialization=True\n",
    "        )\n",
    "        docsearch.add_texts(texts, metadatas=metadatas)\n",
    "    else:\n",
    "        docsearch = FAISS.from_texts(texts, embedding=embeddings_model, metadatas=metadatas)\n",
    "\n",
    "    docsearch.save_local(folder_path=dest_embed_dir)\n",
    "\n",
    "# Function to process HTML, PDF, and other files and create embeddings\n",
    "def create_embeddings(embeddings_model, embedding_path: str = \"./embed\"):\n",
    "    print(f\"Storing embeddings to {embedding_path}\")\n",
    "\n",
    "    documents = []\n",
    "    total_html_docs = 0  # Variable to keep track of HTML document count\n",
    "\n",
    "    # Load HTML content from URLs\n",
    "    for url in urls:\n",
    "        document_content = html_document_loader(url)\n",
    "        if document_content:\n",
    "            total_html_docs += 1\n",
    "            documents.append({'page_content': document_content, 'metadata': {'type': 'html', 'source': url}})\n",
    "\n",
    "    # Extract text from PDFs\n",
    "    pdf_documents = extract_text_from_pdfs(pdf_dir)\n",
    "    total_pdf_docs = len(pdf_documents)  # Count PDF documents\n",
    "    documents.extend(pdf_documents)\n",
    "\n",
    "    # Check if the directory for other files exists and is not empty\n",
    "    if os.path.exists(other_files_dir) and os.listdir(other_files_dir):\n",
    "        other_files_documents = extract_text_from_other_files(other_files_dir)\n",
    "        documents.extend(other_files_documents)\n",
    "\n",
    "    # Extract the text content for splitting\n",
    "    document_texts = [doc['page_content'] for doc in documents if isinstance(doc.get('page_content', \"\"), str)]\n",
    "\n",
    "    # Split text into chunks and create embeddings\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=0,\n",
    "        length_function=len,\n",
    "    )\n",
    "    print(f\"Total HTML documents: {total_html_docs}\")\n",
    "    print(f\"Total PDF documents: {total_pdf_docs}\")\n",
    "    print(f\"Total other documents: {len(documents) - total_html_docs - total_pdf_docs}\")\n",
    "    print(f\"Total documents: {len(document_texts)}\")\n",
    "\n",
    "    texts = text_splitter.create_documents(document_texts)  # Pass the list of strings for splitting\n",
    "    print(f\"Total texts (chunks): {len(texts)}\")\n",
    "\n",
    "    index_docs(embeddings_model, urls, text_splitter, texts, embedding_path)\n",
    "    print(\"Generated embedding successfully\")\n",
    "def create_embeddings(embeddings_model, embedding_path: str = \"./embed\"):\n",
    "    print(f\"Storing embeddings to {embedding_path}\")\n",
    "\n",
    "    documents = []\n",
    "    total_html_docs = 0  # Variable to keep track of HTML document count\n",
    "\n",
    "    # Load HTML content from URLs\n",
    "    for url in urls:\n",
    "        document_content = html_document_loader(url)\n",
    "        if document_content:\n",
    "            total_html_docs += 1\n",
    "            documents.append({'page_content': document_content, 'metadata': {'type': 'html', 'source': url}})\n",
    "\n",
    "    # Extract text from PDFs\n",
    "    pdf_documents = extract_text_from_pdfs(pdf_dir)\n",
    "    total_pdf_docs = len(pdf_documents)  # Count PDF documents\n",
    "    documents.extend(pdf_documents)\n",
    "\n",
    "    # Check if the directory for other files exists and is not empty\n",
    "    if os.path.exists(other_files_dir) and os.listdir(other_files_dir):\n",
    "        other_files_documents = extract_text_from_other_files(other_files_dir)\n",
    "        documents.extend(other_files_documents)\n",
    "\n",
    "    # Extract the text content for splitting\n",
    "    document_texts = [doc['page_content'] for doc in documents if isinstance(doc.get('page_content', \"\"), str)]\n",
    "\n",
    "    # Split text into chunks and create embeddings\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=0,\n",
    "        length_function=len,\n",
    "    )\n",
    "    print(f\"Total HTML documents: {total_html_docs}\")\n",
    "    print(f\"Total PDF documents: {total_pdf_docs}\")\n",
    "    print(f\"Total other documents: {len(documents) - total_html_docs - total_pdf_docs}\")\n",
    "    print(f\"Total documents: {len(document_texts)}\")\n",
    "\n",
    "    texts = text_splitter.create_documents(document_texts)  # Pass the list of strings for splitting\n",
    "    print(f\"Total texts (chunks): {len(texts)}\")\n",
    "\n",
    "    index_docs(embeddings_model, urls, text_splitter, texts, embedding_path)\n",
    "    print(\"Generated embedding successfully\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "75693417-6d64-4f02-aa62-69e5df3181bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storing embeddings to ./embed\n",
      "Processing PDF: 9781788996242_ColorImages.pdf\n",
      "Processing PDF: CUDA_C_Programming_Guide.pdf\n",
      "Processing PDF: [2013] The_CUDA_Handbook.pdf\n",
      "Processing PDF: sc11-cuda-c-basics.pdf\n",
      "Processing PDF: openacc-guide.pdf\n",
      "Processing PDF: CUDA_by_Example.pdf\n",
      "Total HTML documents: 18\n",
      "Total PDF documents: 6\n",
      "Total other documents: 0\n",
      "Total documents: 24\n",
      "Total texts (chunks): 6915\n",
      "Generated embedding successfully\n",
      "CPU times: user 7.08 s, sys: 1.2 s, total: 8.29 s\n",
      "Wall time: 33.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "embeddings_model = NVIDIAEmbeddings(base_url=\"http://0.0.0.0:11022/v1\", model='nvidia/nv-embedqa-e5-v5')\n",
    "create_embeddings(embeddings_model=embeddings_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "49d72381-59c8-486a-b5d3-461616320c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE                                                COMMAND                  CREATED        STATUS        PORTS                                         NAMES        SIZE\n",
      "4ad7b8ee2bb5   nvcr.io/nim/nvidia/nv-embedqa-e5-v5:1.0.1            \"/opt/nvidia/nvidia_…\"   21 hours ago   Up 21 hours   0.0.0.0:11022->8000/tcp, :::11022->8000/tcp   embed_nim    995kB (virtual 15.7GB)\n",
      "6ef6ad73da55   nvcr.io/nim/nvidia/nv-rerankqa-mistral-4b-v3:1.0.2   \"/opt/nvidia/nvidia_…\"   21 hours ago   Up 21 hours   0.0.0.0:11737->8000/tcp, :::11737->8000/tcp   rerank_nim   1.91MB (virtual 15.5GB)\n",
      "a5732a8ccde9   nvcr.io/nim/meta/llama-3.1-8b-instruct:1.1.2         \"/opt/nvidia/nvidia_…\"   22 hours ago   Up 22 hours   0.0.0.0:8000->8000/tcp, :::8000->8000/tcp     LLM_nim      9.03MB (virtual 12.9GB)\n"
     ]
    }
   ],
   "source": [
    "!docker ps -s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e559b733-90e8-4717-aae5-d36cef9376ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untagged: nvcr.io/nim/meta/llama3-8b-instruct:1.0.0\n",
      "Untagged: nvcr.io/nim/meta/llama3-8b-instruct@sha256:7fe6071923b547edd9fba87c891a362ea0b4a88794b8a422d63127e54caa6ef7\n",
      "Deleted: sha256:3cb29b0d79e6d84ca4fd132aa66b652408e8c0fb88177cfee06a55418fdc3de2\n",
      "Deleted: sha256:d363b236bc57a7c2d43748a0272361d9c93a1a830b1b65729d48070734919bf4\n",
      "Deleted: sha256:0afef4f6b6a5eae810e2dde7db5d571d9ac4ac120f1ab8938b6d25ba923d85b4\n",
      "Deleted: sha256:113fe49b1d137ede04be15d2eca24351c065822d01a5a6c65410a3b37660784e\n",
      "Deleted: sha256:90cc92b781e2986bdfff11a03227a7133a237d376e6ae3ada86d28cf710effac\n",
      "Deleted: sha256:c94eb581f0fdd36f8419033ec5c3791925899c00c75638d88ce8638e959df5c6\n",
      "Deleted: sha256:419a712e152882383be969cd316bbb38cb4e1d8191e346a2173cf79210e959a1\n",
      "Deleted: sha256:9c94d94a89cb580e6f0a302668e3330e2dc3461f5db6048bca23f7716735c89b\n",
      "Deleted: sha256:3ae37cc3ebe01ff609d1829e67fcf8b1d9d51696e49e00d4ab154e1c585a007d\n",
      "Deleted: sha256:0e876143f4822a81e5fa5d0f89f8e0c13f3c5f5176dd9f51835a445292bacbe8\n",
      "Deleted: sha256:57a6af77038d2018e7898c13d29051f175d47cc4741b73e35a8f498fdc3e1381\n",
      "Deleted: sha256:fcce2c5a77b1bac6d4a647980540dbe92d7e84f4dc0c748b45321e2c89aad87d\n",
      "Deleted: sha256:13a1af48af37d5ad641dbce098fd0760ac834431d41ad09d43b742ec1cdcb7ba\n",
      "Deleted: sha256:33429a6f81db8c4fc44fe85439582e5a3530238a2eb99aea59a7a061b9726405\n",
      "Deleted: sha256:8f564c60ae9c64d2ff3248a48adfdd285dd8856f2ef13bee35c1c1fe4e93e1ea\n",
      "Deleted: sha256:a8dc6959f9a6949012f09441705fd90a24f5a4fdcee06c4bf500cf989f226a26\n",
      "Deleted: sha256:6dee6361a78dbf7fe97064d81f444f5748cd28736ba1b3b2f77383cd929296dd\n",
      "Deleted: sha256:e32279784004cbf54722e412e106645059c6b070a30c035886fe59c191fc4aa5\n",
      "Deleted: sha256:3f819aa851120c9a0a759125b46eeba7b4891d536474be4639b8f1ed1d1578f6\n",
      "Deleted: sha256:51cab3e16a5ff135e323c70de0769bbb97b3bf9118827220613cfec01833cdab\n",
      "Deleted: sha256:43419bb17b86dd00d56b0f2b3aa9132dd59e9d70c6546ca6f176a6baa720cd48\n",
      "Deleted: sha256:d797a4fc885237240cb2f522e9617988c1dce5bf08640d05abbb69e64fd497cd\n"
     ]
    }
   ],
   "source": [
    "!docker rmi 3cb29b0d79e6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b384c51-0d3c-496b-bda1-761efdf44901",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_path = \"./embed/\"\n",
    "docsearch = FAISS.load_local(folder_path=embedding_path, embeddings=embeddings_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed868c58-e6b3-4fa2-8347-7917869db5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatNVIDIA(base_url= \"http://0.0.0.0:8000/v1\", model=\"meta/llama-3.1-8b-instruct\", temperature=0.1, max_tokens=1000, top_p=1.0)\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "qa_prompt=QA_PROMPT\n",
    "\n",
    "doc_chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=QA_PROMPT)\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=docsearch.as_retriever(),\n",
    "    chain_type=\"stuff\",\n",
    "    memory=memory,\n",
    "    combine_docs_chain_kwargs={'prompt': qa_prompt},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2815e668-bb58-4b11-9b79-c23c0ce7a040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It seems like there's a bit of repetition in the text! Based on the provided context, I'll summarize the salient features of CUDA:\n",
      "\n",
      "1. **No knowledge of OpenGL or DirectX required**: Users don't need to have expertise in these graphics programming interfaces to use CUDA.\n",
      "2. **No need to force problems to look like computer graphics tasks**: CUDA allows users to focus on their specific problem domain without having to adapt it to a graphics-related framework.\n",
      "3. **Orders-of-magnitude performance improvement**: CUDA can lead to significant performance enhancements over traditional approaches.\n",
      "\n",
      "Let me know if you'd like me to clarify or expand on these points!\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the salient features of CUDA\"\n",
    "result = qa({\"question\": query})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e6622fdd-6d58-442d-b037-a03a36ef8e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't have the text to answer the question. It looks like the text is repeated multiple times, but it doesn't provide any content.\n"
     ]
    }
   ],
   "source": [
    "query = \"What is RAG?\"\n",
    "result = qa({\"question\": query})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8502cdd6-48fa-441a-98cf-8fc43fef6719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text doesn't provide a direct answer to the question of how to profile CUDA applications, but it does mention that \"See the Application Note on CUDA for Tegra for details.\" This suggests that the Application Note on CUDA for Tegra contains information on how to profile CUDA applications.\n",
      "\n",
      "So, my answer is: See the Application Note on CUDA for Tegra for details.\n"
     ]
    }
   ],
   "source": [
    "query = \"how to do profiling?\"\n",
    "result = qa({\"question\": query})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5c07a083-99e9-4763-b426-201397798b80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfortunately, the provided text does not explicitly state whether you can read double precision floats from a texture or not. It discusses interpolation and precision in the context of a microdemo, but it does not provide a clear answer to the question.\n"
     ]
    }
   ],
   "source": [
    "query = \"Can I read double precision floats from texture?\"\n",
    "result = qa({\"question\": query})\n",
    "print(result.get(\"answer\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e2cd9d4b-188e-4c04-b567-eff752183cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nemoguardrails in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (0.10.1)\n",
      "Requirement already satisfied: aiohttp>=3.9.2 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from nemoguardrails) (3.9.5)\n",
      "Requirement already satisfied: annoy>=1.17.3 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from nemoguardrails) (1.17.3)\n",
      "Requirement already satisfied: fastapi>=0.103.0 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from nemoguardrails) (0.115.0)\n",
      "Requirement already satisfied: fastembed>=0.2.2 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from nemoguardrails) (0.3.6)\n",
      "Requirement already satisfied: httpx>=0.24.1 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from nemoguardrails) (0.27.0)\n",
      "Requirement already satisfied: jinja2>=3.1.4 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from nemoguardrails) (3.1.4)\n",
      "Requirement already satisfied: langchain!=0.1.9,<0.3.0,>=0.2.14 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from nemoguardrails) (0.2.16)\n",
      "Requirement already satisfied: langchain-core!=0.1.26,<0.3.0,>=0.2.14 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from nemoguardrails) (0.2.41)\n",
      "Requirement already satisfied: langchain-community<0.3.0,>=0.0.16 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from nemoguardrails) (0.2.10)\n",
      "Requirement already satisfied: lark~=1.1.7 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from nemoguardrails) (1.1.9)\n",
      "Requirement already satisfied: nest-asyncio>=1.5.6 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from nemoguardrails) (1.6.0)\n",
      "Requirement already satisfied: prompt-toolkit>=3.0 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from nemoguardrails) (3.0.47)\n",
      "Requirement already satisfied: pydantic>=1.10 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from nemoguardrails) (2.8.2)\n",
      "Requirement already satisfied: pyyaml>=6.0 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from nemoguardrails) (6.0.1)\n",
      "Requirement already satisfied: rich>=13.5.2 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from nemoguardrails) (13.9.2)\n",
      "Requirement already satisfied: simpleeval>=0.9.13 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from nemoguardrails) (1.0.0)\n",
      "Requirement already satisfied: starlette>=0.27.0 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from nemoguardrails) (0.38.6)\n",
      "Requirement already satisfied: typer>=0.7.0 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from nemoguardrails) (0.12.5)\n",
      "Requirement already satisfied: uvicorn>=0.23 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from nemoguardrails) (0.31.0)\n",
      "Requirement already satisfied: watchdog>=3.0.0 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from nemoguardrails) (5.0.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from aiohttp>=3.9.2->nemoguardrails) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from aiohttp>=3.9.2->nemoguardrails) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from aiohttp>=3.9.2->nemoguardrails) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from aiohttp>=3.9.2->nemoguardrails) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from aiohttp>=3.9.2->nemoguardrails) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from aiohttp>=3.9.2->nemoguardrails) (4.0.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from fastapi>=0.103.0->nemoguardrails) (4.12.2)\n",
      "Requirement already satisfied: PyStemmer<3.0.0,>=2.2.0 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from fastembed>=0.2.2->nemoguardrails) (2.2.0.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.20 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from fastembed>=0.2.2->nemoguardrails) (0.25.1)\n",
      "Requirement already satisfied: loguru<0.8.0,>=0.7.2 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from fastembed>=0.2.2->nemoguardrails) (0.7.2)\n",
      "Requirement already satisfied: mmh3<5.0,>=4.0 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from fastembed>=0.2.2->nemoguardrails) (4.1.0)\n",
      "Requirement already satisfied: numpy<2,>=1.21 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from fastembed>=0.2.2->nemoguardrails) (1.26.4)\n",
      "Requirement already satisfied: onnx<2.0.0,>=1.15.0 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from fastembed>=0.2.2->nemoguardrails) (1.17.0)\n",
      "Requirement already satisfied: onnxruntime<2.0.0,>=1.17.0 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from fastembed>=0.2.2->nemoguardrails) (1.19.2)\n",
      "Requirement already satisfied: pillow<11.0.0,>=10.3.0 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from fastembed>=0.2.2->nemoguardrails) (10.4.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.31 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from fastembed>=0.2.2->nemoguardrails) (2.32.3)\n",
      "Requirement already satisfied: snowballstemmer<3.0.0,>=2.2.0 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from fastembed>=0.2.2->nemoguardrails) (2.2.0)\n",
      "Requirement already satisfied: tokenizers<1.0,>=0.15 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from fastembed>=0.2.2->nemoguardrails) (0.20.0)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.66 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from fastembed>=0.2.2->nemoguardrails) (4.66.5)\n",
      "Requirement already satisfied: anyio in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from httpx>=0.24.1->nemoguardrails) (4.4.0)\n",
      "Requirement already satisfied: certifi in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from httpx>=0.24.1->nemoguardrails) (2024.7.4)\n",
      "Requirement already satisfied: httpcore==1.* in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from httpx>=0.24.1->nemoguardrails) (1.0.5)\n",
      "Requirement already satisfied: idna in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from httpx>=0.24.1->nemoguardrails) (3.7)\n",
      "Requirement already satisfied: sniffio in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from httpx>=0.24.1->nemoguardrails) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from httpcore==1.*->httpx>=0.24.1->nemoguardrails) (0.14.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from jinja2>=3.1.4->nemoguardrails) (2.1.5)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from langchain!=0.1.9,<0.3.0,>=0.2.14->nemoguardrails) (2.0.31)\n",
      "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from langchain!=0.1.9,<0.3.0,>=0.2.14->nemoguardrails) (0.2.2)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from langchain!=0.1.9,<0.3.0,>=0.2.14->nemoguardrails) (0.1.131)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from langchain!=0.1.9,<0.3.0,>=0.2.14->nemoguardrails) (8.5.0)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from langchain-community<0.3.0,>=0.0.16->nemoguardrails) (0.6.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from langchain-core!=0.1.26,<0.3.0,>=0.2.14->nemoguardrails) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from langchain-core!=0.1.26,<0.3.0,>=0.2.14->nemoguardrails) (24.1)\n",
      "Requirement already satisfied: wcwidth in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from prompt-toolkit>=3.0->nemoguardrails) (0.2.13)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from pydantic>=1.10->nemoguardrails) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.20.1 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from pydantic>=1.10->nemoguardrails) (2.20.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from rich>=13.5.2->nemoguardrails) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from rich>=13.5.2->nemoguardrails) (2.18.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from typer>=0.7.0->nemoguardrails) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from typer>=0.7.0->nemoguardrails) (1.5.4)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from anyio->httpx>=0.24.1->nemoguardrails) (1.2.2)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.0.16->nemoguardrails) (3.21.3)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.0.16->nemoguardrails) (0.9.0)\n",
      "Requirement already satisfied: filelock in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.20->fastembed>=0.2.2->nemoguardrails) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.20->fastembed>=0.2.2->nemoguardrails) (2024.6.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core!=0.1.26,<0.3.0,>=0.2.14->nemoguardrails) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from langsmith<0.2.0,>=0.1.17->langchain!=0.1.9,<0.3.0,>=0.2.14->nemoguardrails) (3.10.6)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from langsmith<0.2.0,>=0.1.17->langchain!=0.1.9,<0.3.0,>=0.2.14->nemoguardrails) (1.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from markdown-it-py>=2.2.0->rich>=13.5.2->nemoguardrails) (0.1.2)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from onnx<2.0.0,>=1.15.0->fastembed>=0.2.2->nemoguardrails) (5.28.2)\n",
      "Requirement already satisfied: coloredlogs in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from onnxruntime<2.0.0,>=1.17.0->fastembed>=0.2.2->nemoguardrails) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from onnxruntime<2.0.0,>=1.17.0->fastembed>=0.2.2->nemoguardrails) (24.3.25)\n",
      "Requirement already satisfied: sympy in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from onnxruntime<2.0.0,>=1.17.0->fastembed>=0.2.2->nemoguardrails) (1.13.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from requests<3.0,>=2.31->fastembed>=0.2.2->nemoguardrails) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from requests<3.0,>=2.31->fastembed>=0.2.2->nemoguardrails) (2.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain!=0.1.9,<0.3.0,>=0.2.14->nemoguardrails) (3.0.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community<0.3.0,>=0.0.16->nemoguardrails) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from coloredlogs->onnxruntime<2.0.0,>=1.17.0->fastembed>=0.2.2->nemoguardrails) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/gsh-3atzc7/.conda/envs/nim_env/lib/python3.9/site-packages (from sympy->onnxruntime<2.0.0,>=1.17.0->fastembed>=0.2.2->nemoguardrails) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install nemoguardrails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "32160bf5-8d3f-4b87-a1f1-0a0164bd9de9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for RailsConfig\nmodels\n  Field required [type=missing, input_value={'instructions': [{'type'.../workspace/config.yaml'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.8/v/missing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnemoguardrails\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LLMRails, RailsConfig\n\u001b[0;32m----> 3\u001b[0m config \u001b[38;5;241m=\u001b[39m \u001b[43mRailsConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/mnt/lustre/hackathons/hack_teams/hack_team_16/workspace/config.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m rails \u001b[38;5;241m=\u001b[39m LLMRails(config)\n",
      "File \u001b[0;32m~/.conda/envs/nim_env/lib/python3.9/site-packages/nemoguardrails/rails/llm/config.py:984\u001b[0m, in \u001b[0;36mRailsConfig.from_path\u001b[0;34m(cls, config_path)\u001b[0m\n\u001b[1;32m    980\u001b[0m     raw_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstructions\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m _default_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstructions\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    982\u001b[0m raw_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig_path\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_path\n\u001b[0;32m--> 984\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/nim_env/lib/python3.9/site-packages/nemoguardrails/rails/llm/config.py:1055\u001b[0m, in \u001b[0;36mRailsConfig.parse_object\u001b[0;34m(cls, obj)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m flow_data\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melements\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m flow_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melements\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1051\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_type\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1052\u001b[0m         ):\n\u001b[1;32m   1053\u001b[0m             flow_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melements\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m parse_flow_elements(flow_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124melements\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m-> 1055\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_obj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/nim_env/lib/python3.9/site-packages/pydantic/main.py:1135\u001b[0m, in \u001b[0;36mBaseModel.parse_obj\u001b[0;34m(cls, obj)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m   1130\u001b[0m \u001b[38;5;129m@typing_extensions\u001b[39m\u001b[38;5;241m.\u001b[39mdeprecated(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe `parse_obj` method is deprecated; use `model_validate` instead.\u001b[39m\u001b[38;5;124m'\u001b[39m, category\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_obj\u001b[39m(\u001b[38;5;28mcls\u001b[39m, obj: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:  \u001b[38;5;66;03m# noqa: D102\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThe `parse_obj` method is deprecated; use `model_validate` instead.\u001b[39m\u001b[38;5;124m'\u001b[39m, category\u001b[38;5;241m=\u001b[39mPydanticDeprecatedSince20\n\u001b[1;32m   1134\u001b[0m     )\n\u001b[0;32m-> 1135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel_validate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/nim_env/lib/python3.9/site-packages/pydantic/main.py:568\u001b[0m, in \u001b[0;36mBaseModel.model_validate\u001b[0;34m(cls, obj, strict, from_attributes, context)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    567\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 568\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstrict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_attributes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfrom_attributes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for RailsConfig\nmodels\n  Field required [type=missing, input_value={'instructions': [{'type'.../workspace/config.yaml'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.8/v/missing"
     ]
    }
   ],
   "source": [
    "from nemoguardrails import LLMRails, RailsConfig\n",
    "\n",
    "config = RailsConfig.from_path(\"/mnt/lustre/hackathons/hack_teams/hack_team_16/workspace/config.yaml\")\n",
    "rails = LLMRails(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5a2e7719-21af-4342-bb5f-708a8e6b864f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"Id\": \"a5732a8ccde9ca1fbf867980c23f1dd95424b2072b1f6f77c6f408aa39f48ad5\",\n",
      "        \"Created\": \"2024-10-04T07:31:05.165531824Z\",\n",
      "        \"Path\": \"/opt/nvidia/nvidia_entrypoint.sh\",\n",
      "        \"Args\": [\n",
      "            \"/opt/nim/start-server.sh\"\n",
      "        ],\n",
      "        \"State\": {\n",
      "            \"Status\": \"running\",\n",
      "            \"Running\": true,\n",
      "            \"Paused\": false,\n",
      "            \"Restarting\": false,\n",
      "            \"OOMKilled\": false,\n",
      "            \"Dead\": false,\n",
      "            \"Pid\": 586536,\n",
      "            \"ExitCode\": 0,\n",
      "            \"Error\": \"\",\n",
      "            \"StartedAt\": \"2024-10-04T07:31:05.613158479Z\",\n",
      "            \"FinishedAt\": \"0001-01-01T00:00:00Z\"\n",
      "        },\n",
      "        \"Image\": \"sha256:c9102c3e95f3acae5b41fdafd7aefd3389bdaf33cf0b3458041e45046bf6bb1d\",\n",
      "        \"ResolvConfPath\": \"/raid/tmp/docker-container-storage-2072/containers/a5732a8ccde9ca1fbf867980c23f1dd95424b2072b1f6f77c6f408aa39f48ad5/resolv.conf\",\n",
      "        \"HostnamePath\": \"/raid/tmp/docker-container-storage-2072/containers/a5732a8ccde9ca1fbf867980c23f1dd95424b2072b1f6f77c6f408aa39f48ad5/hostname\",\n",
      "        \"HostsPath\": \"/raid/tmp/docker-container-storage-2072/containers/a5732a8ccde9ca1fbf867980c23f1dd95424b2072b1f6f77c6f408aa39f48ad5/hosts\",\n",
      "        \"LogPath\": \"/raid/tmp/docker-container-storage-2072/containers/a5732a8ccde9ca1fbf867980c23f1dd95424b2072b1f6f77c6f408aa39f48ad5/a5732a8ccde9ca1fbf867980c23f1dd95424b2072b1f6f77c6f408aa39f48ad5-json.log\",\n",
      "        \"Name\": \"/LLM_nim\",\n",
      "        \"RestartCount\": 0,\n",
      "        \"Driver\": \"overlay2\",\n",
      "        \"Platform\": \"linux\",\n",
      "        \"MountLabel\": \"\",\n",
      "        \"ProcessLabel\": \"\",\n",
      "        \"AppArmorProfile\": \"\",\n",
      "        \"ExecIDs\": [\n",
      "            \"8d31b67e5ae24a6dc5cadec7ab66187db91c119904499fdb491f49231b53a136\"\n",
      "        ],\n",
      "        \"HostConfig\": {\n",
      "            \"Binds\": [\n",
      "                \"/home/gsh-3atzc7/.cache/nim:/opt/nim/.cache\"\n",
      "            ],\n",
      "            \"ContainerIDFile\": \"\",\n",
      "            \"LogConfig\": {\n",
      "                \"Type\": \"json-file\",\n",
      "                \"Config\": {}\n",
      "            },\n",
      "            \"NetworkMode\": \"default\",\n",
      "            \"PortBindings\": {\n",
      "                \"8000/tcp\": [\n",
      "                    {\n",
      "                        \"HostIp\": \"\",\n",
      "                        \"HostPort\": \"8000\"\n",
      "                    }\n",
      "                ]\n",
      "            },\n",
      "            \"RestartPolicy\": {\n",
      "                \"Name\": \"no\",\n",
      "                \"MaximumRetryCount\": 0\n",
      "            },\n",
      "            \"AutoRemove\": true,\n",
      "            \"VolumeDriver\": \"\",\n",
      "            \"VolumesFrom\": null,\n",
      "            \"ConsoleSize\": [\n",
      "                24,\n",
      "                80\n",
      "            ],\n",
      "            \"CapAdd\": null,\n",
      "            \"CapDrop\": null,\n",
      "            \"CgroupnsMode\": \"host\",\n",
      "            \"Dns\": [],\n",
      "            \"DnsOptions\": [],\n",
      "            \"DnsSearch\": [],\n",
      "            \"ExtraHosts\": null,\n",
      "            \"GroupAdd\": null,\n",
      "            \"IpcMode\": \"private\",\n",
      "            \"Cgroup\": \"\",\n",
      "            \"Links\": null,\n",
      "            \"OomScoreAdj\": 0,\n",
      "            \"PidMode\": \"\",\n",
      "            \"Privileged\": false,\n",
      "            \"PublishAllPorts\": false,\n",
      "            \"ReadonlyRootfs\": false,\n",
      "            \"SecurityOpt\": null,\n",
      "            \"UTSMode\": \"\",\n",
      "            \"UsernsMode\": \"\",\n",
      "            \"ShmSize\": 17179869184,\n",
      "            \"Runtime\": \"runc\",\n",
      "            \"Isolation\": \"\",\n",
      "            \"CpuShares\": 0,\n",
      "            \"Memory\": 0,\n",
      "            \"NanoCpus\": 0,\n",
      "            \"CgroupParent\": \"\",\n",
      "            \"BlkioWeight\": 0,\n",
      "            \"BlkioWeightDevice\": [],\n",
      "            \"BlkioDeviceReadBps\": [],\n",
      "            \"BlkioDeviceWriteBps\": [],\n",
      "            \"BlkioDeviceReadIOps\": [],\n",
      "            \"BlkioDeviceWriteIOps\": [],\n",
      "            \"CpuPeriod\": 0,\n",
      "            \"CpuQuota\": 0,\n",
      "            \"CpuRealtimePeriod\": 0,\n",
      "            \"CpuRealtimeRuntime\": 0,\n",
      "            \"CpusetCpus\": \"\",\n",
      "            \"CpusetMems\": \"\",\n",
      "            \"Devices\": [],\n",
      "            \"DeviceCgroupRules\": null,\n",
      "            \"DeviceRequests\": [\n",
      "                {\n",
      "                    \"Driver\": \"\",\n",
      "                    \"Count\": 1,\n",
      "                    \"DeviceIDs\": null,\n",
      "                    \"Capabilities\": [\n",
      "                        [\n",
      "                            \"gpu\"\n",
      "                        ]\n",
      "                    ],\n",
      "                    \"Options\": {}\n",
      "                }\n",
      "            ],\n",
      "            \"MemoryReservation\": 0,\n",
      "            \"MemorySwap\": 0,\n",
      "            \"MemorySwappiness\": null,\n",
      "            \"OomKillDisable\": false,\n",
      "            \"PidsLimit\": null,\n",
      "            \"Ulimits\": null,\n",
      "            \"CpuCount\": 0,\n",
      "            \"CpuPercent\": 0,\n",
      "            \"IOMaximumIOps\": 0,\n",
      "            \"IOMaximumBandwidth\": 0,\n",
      "            \"MaskedPaths\": [\n",
      "                \"/proc/asound\",\n",
      "                \"/proc/acpi\",\n",
      "                \"/proc/kcore\",\n",
      "                \"/proc/keys\",\n",
      "                \"/proc/latency_stats\",\n",
      "                \"/proc/timer_list\",\n",
      "                \"/proc/timer_stats\",\n",
      "                \"/proc/sched_debug\",\n",
      "                \"/proc/scsi\",\n",
      "                \"/sys/firmware\",\n",
      "                \"/sys/devices/virtual/powercap\"\n",
      "            ],\n",
      "            \"ReadonlyPaths\": [\n",
      "                \"/proc/bus\",\n",
      "                \"/proc/fs\",\n",
      "                \"/proc/irq\",\n",
      "                \"/proc/sys\",\n",
      "                \"/proc/sysrq-trigger\"\n",
      "            ]\n",
      "        },\n",
      "        \"GraphDriver\": {\n",
      "            \"Data\": {\n",
      "                \"LowerDir\": \"/raid/tmp/docker-container-storage-2072/overlay2/7fa8bad39ef73b8974ba6ee32e6d5376d67f47208c9a0b527c07d01ff13856a8-init/diff:/raid/tmp/docker-container-storage-2072/overlay2/f669aa87c3eb41bad93ae3a130baf6580cc2cc1426b336ea012f9e93d1ae4c3e/diff:/raid/tmp/docker-container-storage-2072/overlay2/75f517e56e76f2f1ca9833270832f308fb5e31fccfa50507330f51dd52c88c14/diff:/raid/tmp/docker-container-storage-2072/overlay2/2ef63166cb032edca8aee744980f0a18c8026a4dde5f8064141cc8bc481396ba/diff:/raid/tmp/docker-container-storage-2072/overlay2/0d9e54f481a4581b520f018dc75ee8cccc70f2c7fdb9fb3c189e4355e7d6c086/diff:/raid/tmp/docker-container-storage-2072/overlay2/76b9c6cf4cba22f91cc31dfd428b93d8f3aa14945505d28151fb2f4704d37cb4/diff:/raid/tmp/docker-container-storage-2072/overlay2/d3b5f54e19dfb527acf90f73edc2aa57661389403eb2183e98dbb8417e032d04/diff:/raid/tmp/docker-container-storage-2072/overlay2/9768f8973c8f85e8b9d72b902475cbeac433be43e7a728940c7bade3d17f4ca9/diff:/raid/tmp/docker-container-storage-2072/overlay2/f6698fb1397edec9df385d0c730537fc88f5a070eb230bfe0b948132b7b3409b/diff:/raid/tmp/docker-container-storage-2072/overlay2/5ab59d65d6a2f332e442461567ff9ecffead036b39482485ce171149706e05b4/diff:/raid/tmp/docker-container-storage-2072/overlay2/15f20fbbd141b4cbe15fc2a9f52d16c8f9c7954eb909b10514786ed053e7cbb7/diff:/raid/tmp/docker-container-storage-2072/overlay2/bd3cba5ec202614a981c86b431da1d193cf65640e7c7da64d1d0a0a2781b40c8/diff:/raid/tmp/docker-container-storage-2072/overlay2/d1d1811631c982c9b75e27d26f8c28e2952bacd7be8a80b14e43512904bda202/diff:/raid/tmp/docker-container-storage-2072/overlay2/40821489442b0324f07d70f1e31612dba525be219ac00bd8e547cb09e6a7aa39/diff:/raid/tmp/docker-container-storage-2072/overlay2/02d228e1837a7993a0cc8ff94cdfd154b3cd34ddf2908b27164c70e7aa801cc4/diff:/raid/tmp/docker-container-storage-2072/overlay2/e73f06b6765142a91b81bc1f9233515e8e01e41787a1df659f2f81056322d2b4/diff:/raid/tmp/docker-container-storage-2072/overlay2/02a790d8fd421ac02885493045ad3fff255ef57b02d981dd99df48b25d19c1be/diff:/raid/tmp/docker-container-storage-2072/overlay2/8f3ace0fd768efb8de1c5f51ab846f41f5acb65e2ec293b9186cd051fd4e9c51/diff:/raid/tmp/docker-container-storage-2072/overlay2/4954a46a43fec797c5540e65f615a1e83a0ebbc2beaa9abb122f39f6428765fc/diff:/raid/tmp/docker-container-storage-2072/overlay2/093337cddc8e5a2d89b43396d4def56af21c721e5f870003029d6992641ef76d/diff:/raid/tmp/docker-container-storage-2072/overlay2/405f86510649d2e6d116a9e6fbf95b7dcd9e894b876a0f5e5df171aa1a4e8943/diff:/raid/tmp/docker-container-storage-2072/overlay2/84be52f435919bbf7461747d5c29064bfebd72cc0c9b945fc73f9803e2370bad/diff:/raid/tmp/docker-container-storage-2072/overlay2/37fb2e17b9ed544202365202189345f973dced30befa7834169eabbb63e737da/diff:/raid/tmp/docker-container-storage-2072/overlay2/f5341b5b3286784841fdb2a3e3055476b415c8f5a41d0fa8443858d93b5f877d/diff:/raid/tmp/docker-container-storage-2072/overlay2/1843dc55d484840fbd3974e8a78d07f5b0429554f3d3af77195c122be014b546/diff:/raid/tmp/docker-container-storage-2072/overlay2/20700c9629e228e957bdc7f7b0940a11d17f2b6bd577793531e4be95a10769ee/diff:/raid/tmp/docker-container-storage-2072/overlay2/bf15adb049c673db4671097d8ec574cde2bde62150f0da8533b221593eea7bfa/diff:/raid/tmp/docker-container-storage-2072/overlay2/9e18d5dc67e8447f00ee348e4a55414ed570de1fc89591e80b68504f42c666e7/diff:/raid/tmp/docker-container-storage-2072/overlay2/dc8fe7a93d4be5b014dd98e822a85ca01a8e48b13fa7295f2fb17b9ad4521a86/diff:/raid/tmp/docker-container-storage-2072/overlay2/d5a96a64b7cdb485e5df27b41a61a0e49fa6e93b43d3aec8630f01ccb140e58c/diff\",\n",
      "                \"MergedDir\": \"/raid/tmp/docker-container-storage-2072/overlay2/7fa8bad39ef73b8974ba6ee32e6d5376d67f47208c9a0b527c07d01ff13856a8/merged\",\n",
      "                \"UpperDir\": \"/raid/tmp/docker-container-storage-2072/overlay2/7fa8bad39ef73b8974ba6ee32e6d5376d67f47208c9a0b527c07d01ff13856a8/diff\",\n",
      "                \"WorkDir\": \"/raid/tmp/docker-container-storage-2072/overlay2/7fa8bad39ef73b8974ba6ee32e6d5376d67f47208c9a0b527c07d01ff13856a8/work\"\n",
      "            },\n",
      "            \"Name\": \"overlay2\"\n",
      "        },\n",
      "        \"Mounts\": [\n",
      "            {\n",
      "                \"Type\": \"bind\",\n",
      "                \"Source\": \"/home/gsh-3atzc7/.cache/nim\",\n",
      "                \"Destination\": \"/opt/nim/.cache\",\n",
      "                \"Mode\": \"\",\n",
      "                \"RW\": true,\n",
      "                \"Propagation\": \"rprivate\"\n",
      "            }\n",
      "        ],\n",
      "        \"Config\": {\n",
      "            \"Hostname\": \"a5732a8ccde9\",\n",
      "            \"Domainname\": \"\",\n",
      "            \"User\": \"2072\",\n",
      "            \"AttachStdin\": false,\n",
      "            \"AttachStdout\": false,\n",
      "            \"AttachStderr\": false,\n",
      "            \"ExposedPorts\": {\n",
      "                \"8000/tcp\": {}\n",
      "            },\n",
      "            \"Tty\": true,\n",
      "            \"OpenStdin\": true,\n",
      "            \"StdinOnce\": false,\n",
      "            \"Env\": [\n",
      "                \"NGC_API_KEY=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\",\n",
      "                \"PATH=/opt/nim/llm/.venv/bin:/opt/hpcx/ucc/bin:/opt/hpcx/ucx/bin:/opt/hpcx/ompi/bin:/usr/local/mpi/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/local/ucx/bin\",\n",
      "                \"NVARCH=x86_64\",\n",
      "                \"NVIDIA_REQUIRE_CUDA=cuda>=12.3 brand=tesla,driver>=470,driver<471 brand=unknown,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=geforce,driver>=470,driver<471 brand=geforcertx,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=titan,driver>=470,driver<471 brand=titanrtx,driver>=470,driver<471 brand=tesla,driver>=525,driver<526 brand=unknown,driver>=525,driver<526 brand=nvidia,driver>=525,driver<526 brand=nvidiartx,driver>=525,driver<526 brand=geforce,driver>=525,driver<526 brand=geforcertx,driver>=525,driver<526 brand=quadro,driver>=525,driver<526 brand=quadrortx,driver>=525,driver<526 brand=titan,driver>=525,driver<526 brand=titanrtx,driver>=525,driver<526 brand=tesla,driver>=535,driver<536 brand=unknown,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=geforce,driver>=535,driver<536 brand=geforcertx,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=titan,driver>=535,driver<536 brand=titanrtx,driver>=535,driver<536\",\n",
      "                \"NV_CUDA_CUDART_VERSION=12.3.101-1\",\n",
      "                \"NV_CUDA_COMPAT_PACKAGE=cuda-compat-12-3\",\n",
      "                \"CUDA_VERSION=12.3.1\",\n",
      "                \"LD_LIBRARY_PATH=/opt/hpcx/ucc/lib/ucc:/opt/hpcx/ucc/lib:/opt/hpcx/ucx/lib/ucx:/opt/hpcx/ucx/lib:/opt/hpcx/ompi/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64:/opt/nim/llm/.venv/lib/python3.10/site-packages/tensorrt_llm/libs:/opt/nim/llm/.venv/lib/python3.10/site-packages/nvidia/cublas/lib:/opt/nim/llm/.venv/lib/python3.10/site-packages/tensorrt_libs:/opt/nim/llm/.venv/lib/python3.10/site-packages/nvidia/nccl/lib\",\n",
      "                \"NVIDIA_VISIBLE_DEVICES=all\",\n",
      "                \"NVIDIA_DRIVER_CAPABILITIES=compute,utility\",\n",
      "                \"PYTHONDONTWRITEBYTECODE=1\",\n",
      "                \"PYTHONUNBUFFERED=1\",\n",
      "                \"PIP_DISABLE_PIP_VERSION_CHECK=1\",\n",
      "                \"PIP_CACHE_DIR=/root/.cache/pip\",\n",
      "                \"POETRY_NO_INTERACTION=1\",\n",
      "                \"POETRY_NO_ANSI=1\",\n",
      "                \"POETRY_VERSION=1.8.2\",\n",
      "                \"POETRY_HOME=/opt/poetry\",\n",
      "                \"POETRY_CACHE_DIR=/root/.cache/pypoetry\",\n",
      "                \"POETRY_VIRTUALENVS_IN_PROJECT=1\",\n",
      "                \"LLM_PROJECT_DIR=/opt/nim/llm\",\n",
      "                \"OPAL_PREFIX=/opt/hpcx/ompi\",\n",
      "                \"OMPI_MCA_coll_hcoll_enable=0\",\n",
      "                \"HPCX_DIR=/opt/hpcx\",\n",
      "                \"HPCX_UCX_DIR=/opt/hpcx/ucx\",\n",
      "                \"HPCX_UCC_DIR=/opt/hpcx/ucc\",\n",
      "                \"HPCX_MPI_DIR=/opt/hpcx/ompi\",\n",
      "                \"OMPI_HOME=/opt/hpcx/ompi\",\n",
      "                \"MPIf_HOME=/opt/hpcx/ompi\",\n",
      "                \"LIBRARY_PATH=/opt/hpcx/ucc/lib:/opt/hpcx/ucx/lib:/opt/hpcx/ompi/lib:\",\n",
      "                \"CPATH=/opt/hpcx/ompi/include:/opt/hpcx/ucc/include:/opt/hpcx/ucx/include:\",\n",
      "                \"PKG_CONFIG_PATH=/lib/pkgconfig:/lib/pkgconfig:/opt/hpcx/ucx/lib/pkgconfig:/opt/hpcx/ompi/lib/pkgconfig:\",\n",
      "                \"USE_NCCL=1\",\n",
      "                \"USE_SYSTEM_NCCL=1\",\n",
      "                \"NCCL_ROOT_DIR=/usr/local/cuda/\",\n",
      "                \"USE_UCC=1\",\n",
      "                \"USE_SYSTEM_UCC=1\",\n",
      "                \"UCC_HOME=/opt/hpcx/ucc\",\n",
      "                \"UCC_DIR=/opt/hpcx/ucc/lib/cmake/ucc\",\n",
      "                \"UCX_HOME=/opt/hpcx/ucx\",\n",
      "                \"UCX_DIR=/opt/hpcx/ucx/lib/cmake/ucx\",\n",
      "                \"USE_DISTRIBUTED=1\",\n",
      "                \"NVIDIA_PRODUCT_NAME=NVIDIA Inference Microservice LLM NIM\",\n",
      "                \"FORCE_NCCL_ALL_REDUCE_STRATEGY=1\",\n",
      "                \"INFERENCE_MICROSERVICE_LLM_NIM_VERSION=1.1.2\",\n",
      "                \"NIM_CACHE_PATH=/opt/nim/.cache\",\n",
      "                \"OUTLINES_CACHE_DIR=/tmp/outlines\",\n",
      "                \"NIM_MODEL_NAME=nim/meta/llama-3.1-8b-instruct\",\n",
      "                \"NIM_NSPECT_ID=NSPECT-ZP6P-5LPY\",\n",
      "                \"NIM_MODEL_NSPECT_ID=NSPECT-0DQP-LNLV\"\n",
      "            ],\n",
      "            \"Cmd\": [\n",
      "                \"/opt/nim/start-server.sh\"\n",
      "            ],\n",
      "            \"Image\": \"nvcr.io/nim/meta/llama-3.1-8b-instruct:1.1.2\",\n",
      "            \"Volumes\": null,\n",
      "            \"WorkingDir\": \"\",\n",
      "            \"Entrypoint\": [\n",
      "                \"/opt/nvidia/nvidia_entrypoint.sh\"\n",
      "            ],\n",
      "            \"OnBuild\": null,\n",
      "            \"Labels\": {\n",
      "                \"com.nvidia.nim.model\": \"meta/llama-3.1-8b-instruct\",\n",
      "                \"com.nvidia.nim.model-nspect\": \"NSPECT-0DQP-LNLV\",\n",
      "                \"com.nvidia.nim.nspect\": \"NSPECT-ZP6P-5LPY\",\n",
      "                \"com.nvidia.nim.type\": \"llm\",\n",
      "                \"com.nvidia.nim.version\": \"1.1.2\",\n",
      "                \"maintainer\": \"NVIDIA CORPORATION <cudatools@nvidia.com>\",\n",
      "                \"org.opencontainers.image.ref.name\": \"ubuntu\",\n",
      "                \"org.opencontainers.image.version\": \"22.04\"\n",
      "            }\n",
      "        },\n",
      "        \"NetworkSettings\": {\n",
      "            \"Bridge\": \"\",\n",
      "            \"SandboxID\": \"a92c5188c03c5f709acf1eee5a836c125b0deb0a90b938e85f0a49cd5547fc5e\",\n",
      "            \"HairpinMode\": false,\n",
      "            \"LinkLocalIPv6Address\": \"\",\n",
      "            \"LinkLocalIPv6PrefixLen\": 0,\n",
      "            \"Ports\": {\n",
      "                \"8000/tcp\": [\n",
      "                    {\n",
      "                        \"HostIp\": \"0.0.0.0\",\n",
      "                        \"HostPort\": \"8000\"\n",
      "                    },\n",
      "                    {\n",
      "                        \"HostIp\": \"::\",\n",
      "                        \"HostPort\": \"8000\"\n",
      "                    }\n",
      "                ]\n",
      "            },\n",
      "            \"SandboxKey\": \"/raid/tmp/xdg_runtime_dir_2072/docker/netns/a92c5188c03c\",\n",
      "            \"SecondaryIPAddresses\": null,\n",
      "            \"SecondaryIPv6Addresses\": null,\n",
      "            \"EndpointID\": \"b5d7ecfcd361e86e1379f6364e4db41f18cde01541dac9c4b5a4b6a0d9914ad2\",\n",
      "            \"Gateway\": \"172.17.0.1\",\n",
      "            \"GlobalIPv6Address\": \"\",\n",
      "            \"GlobalIPv6PrefixLen\": 0,\n",
      "            \"IPAddress\": \"172.17.0.2\",\n",
      "            \"IPPrefixLen\": 16,\n",
      "            \"IPv6Gateway\": \"\",\n",
      "            \"MacAddress\": \"02:42:ac:11:00:02\",\n",
      "            \"Networks\": {\n",
      "                \"bridge\": {\n",
      "                    \"IPAMConfig\": null,\n",
      "                    \"Links\": null,\n",
      "                    \"Aliases\": null,\n",
      "                    \"NetworkID\": \"36523888f73dbca5f0be5cd2aad2c19bfe6039a95d9ceee1f7db74bb1e993d76\",\n",
      "                    \"EndpointID\": \"b5d7ecfcd361e86e1379f6364e4db41f18cde01541dac9c4b5a4b6a0d9914ad2\",\n",
      "                    \"Gateway\": \"172.17.0.1\",\n",
      "                    \"IPAddress\": \"172.17.0.2\",\n",
      "                    \"IPPrefixLen\": 16,\n",
      "                    \"IPv6Gateway\": \"\",\n",
      "                    \"GlobalIPv6Address\": \"\",\n",
      "                    \"GlobalIPv6PrefixLen\": 0,\n",
      "                    \"MacAddress\": \"02:42:ac:11:00:02\",\n",
      "                    \"DriverOpts\": null\n",
      "                }\n",
      "            }\n",
      "        }\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "!docker inspect LLM_nim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3af5a0b1-af05-4076-b139-e56139bd5ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   IMAGE                                                COMMAND                  CREATED      STATUS      PORTS                                         NAMES\n",
      "4ad7b8ee2bb5   nvcr.io/nim/nvidia/nv-embedqa-e5-v5:1.0.1            \"/opt/nvidia/nvidia_…\"   4 days ago   Up 4 days   0.0.0.0:11022->8000/tcp, :::11022->8000/tcp   embed_nim\n",
      "6ef6ad73da55   nvcr.io/nim/nvidia/nv-rerankqa-mistral-4b-v3:1.0.2   \"/opt/nvidia/nvidia_…\"   4 days ago   Up 4 days   0.0.0.0:11737->8000/tcp, :::11737->8000/tcp   rerank_nim\n",
      "a5732a8ccde9   nvcr.io/nim/meta/llama-3.1-8b-instruct:1.1.2         \"/opt/nvidia/nvidia_…\"   4 days ago   Up 4 days   0.0.0.0:8000->8000/tcp, :::8000->8000/tcp     LLM_nim\n"
     ]
    }
   ],
   "source": [
    "!docker ps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "766dbfce-e04b-4760-9643-5bdabac1fb16",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever=docsearch.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "57682fde-5fb5-452e-9b8b-7aa7fbf78bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_nvidia_ai_endpoints import NVIDIARerank\n",
    "\n",
    "# Re-initialize and connect to a NeMo Retriever Text Reranking NIM running at localhost:8000\n",
    "compressor = NVIDIARerank(model=\"nvidia/nv-rerankqa-mistral-4b-v3\",\n",
    "                          base_url=\"http://localhost:11737/v1\")\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7a4c6a4e-b4de-478f-9f1a-52a243efdb53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'an I read double precision floats from texture?',\n",
       " 'result': 'No, the provided documentation does not mention support for reading double precision floats from texture. The supported array formats for textures include:\\n\\n* unsigned char (CU_AD_FORMAT_UNSIGNED_INT8)\\n* unsigned short (CU_AD_FORMAT_UNSIGNED_INT16)\\n* unsigned int (CU_AD_FORMAT_UNSIGNED_INT32)\\n* signed char (CU_AD_FORMAT_SIGNED_INT8)\\n* short (CU_AD_FORMAT_SIGNED_INT16)\\n* int (CU_AD_FORMAT_SIGNED_INT32)\\n* half (IEEE 754 “binary16” format) (CU_AD_FORMAT_SIGNED_HALF)\\n* signed float (CU_AD_FORMAT_SIGNED_FLOAT)\\n\\nThere is no mention of double precision float support.'}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "query = \"an I read double precision floats from texture?\"\n",
    "\n",
    "chain = RetrievalQA.from_chain_type(llm=llm, retriever=compression_retriever)\n",
    "chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c46e3711-2199-4655-ab67-4440e497b9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Question: Consider the following code snippet executing on a GPU architecture where i) width of L1 cache line is 64 bytesii) width of L2 cache line is 16 bytesiii) warp size is 16__global__ void mem_access(float *out, float *in){int tidx = blockIdx.x*blockDim.x + threadIdx.x;out[tidx] = in[tidx*4];}Assume that the number of threads being launched is 512. The size of the in array is 2048and the size of the out array is 512. What are the total number of L1 writes to L2 cache?',\n",
       " 'result': \"To determine the total number of L1 writes to L2 cache, we need to understand how memory accesses are handled in the given code.\\n\\nThe kernel function `mem_access` is launched with 512 threads, and the out array has a size of 512. This means that each thread writes to a unique location in the out array.\\n\\nThe in array has a size of 2048, which is larger than the out array. The kernel function accesses the in array with a stride of 4, which means that each thread accesses 4 consecutive elements in the in array.\\n\\nSince the warp size is 16, we can calculate the number of warps launched as follows:\\n\\n512 threads / 16 threads per warp = 32 warps\\n\\nEach warp will access 4 consecutive elements in the in array, which means that each warp will access 4 elements in each iteration. Since the out array has a size of 512, each warp will access the out array once.\\n\\nThe L1 cache line width is 64 bytes, and the size of a float is 4 bytes. This means that each L1 cache line can hold 16 floats (64 bytes / 4 bytes per float).\\n\\nSince each warp accesses 4 floats in each iteration, the number of L1 cache lines accessed by each warp is:\\n\\n4 floats / 16 floats per L1 cache line = 0.25 L1 cache lines\\n\\nSince each warp accesses the out array once, the total number of L1 cache lines accessed by each warp is 1.\\n\\nThe L2 cache line width is 16 bytes, and the size of a float is 4 bytes. This means that each L2 cache line can hold 4 floats (16 bytes / 4 bytes per float).\\n\\nSince each warp accesses 4 floats in each iteration, the number of L2 cache lines accessed by each warp is:\\n\\n4 floats / 4 floats per L2 cache line = 1 L2 cache line\\n\\nNow, let's consider the total number of L1 writes to L2 cache. Since each warp accesses the out array once, and each warp accesses 1 L2 cache line, the total number of L1 writes to L2 cache is equal to the number of warps launched, which is 32.\\n\\nHowever, we need to consider that each thread writes to a unique location in the out array, and the L1 cache line width is 64 bytes, which can hold 16 floats. This means that each thread will write to a separate L1 cache line, and there will be no conflicts between threads.\\n\\nTherefore, the total number of L1 writes to L2 cache is equal to the number of threads launched, which is 512.\\n\\nSo, the total number of L1 writes to L2 cache is 512.\"}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "query = \"how to do\n",
    "chain = RetrievalQA.from_chain_type(llm=llm, retriever=compression_retriever)\n",
    "chain.invoke(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7075e8cb-5196-456f-95a9-75cf6c3c3a95",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
